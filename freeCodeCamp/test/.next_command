builtin unset VSCODE_INJECTION
[ -z "$VSCODE_SHELL_INTEGRATION" ]
[ -n "${VSCODE_ENV_REPLACE:-}" ]
[ -n "${VSCODE_ENV_PREPEND:-}" ]
[ -n "${VSCODE_ENV_APPEND:-}" ]
__vsc_regex_environment="^CYGWIN*|MINGW*|MSYS*"
[[ "$(uname -s)" =~ $__vsc_regex_environment ]]
__vsc_is_windows=0
__vsc_regex_histcontrol=".*(erasedups|ignoreboth|ignoredups).*"
[[ "$HISTCONTROL" =~ $__vsc_regex_histcontrol ]]
__vsc_history_verify=1
builtin unset __vsc_regex_environment
builtin unset __vsc_regex_histcontrol
__vsc_initialized=0
__vsc_original_PS1="$PS1"
__vsc_original_PS2="$PS2"
__vsc_custom_PS1=""
__vsc_custom_PS2=""
__vsc_in_command_execution="1"
__vsc_current_command=""
__vsc_nonce="$VSCODE_NONCE"
unset VSCODE_NONCE
__vsc_stable="$VSCODE_STABLE"
unset VSCODE_STABLE
[ "$__vsc_stable" = "0" ]
[ -n "$STARSHIP_SESSION_KEY" ]
[ -n "$POSH_SESSION_ID" ]
builtin printf '\e]633;P;HasRichCommandDetection=True\a'
[[ -n "${bash_preexec_imported:-}" ]]
__vsc_dbg_trap="$(__vsc_get_trap DEBUG)"
[[ -z "$__vsc_dbg_trap" ]]
[[ "$__vsc_dbg_trap" != '__vsc_preexec "$_"' && "$__vsc_dbg_trap" != '__vsc_preexec_all "$_"' ]]
trap '__vsc_preexec_all "$_"' DEBUG
__vsc_original_prompt_command=${PROMPT_COMMAND:-}
touch my_first_script.py
students = ["Alice", "Bob", "Charlie", "Diana", "Eve"]
touch my_first_script.py
students = ["Alice", "Bob", "Charlie", "Diana", "Eve"]
mkdir -p data/raw data/processed data/sample
find . -type d
./config
cat > src/extract/data_extractor.py <<'EOF' """ Student Feedback Data Extractor =============================== This module handles data extraction from various sources. For this project, we'll create realistic sample data. """ import csv import random import json from datetime import datetime, timedelta from pathlib import Path class StudentFeedbackExtractor: """Extract and generate student feedback data""" def __init__(self, data_dir="data/raw"): self.data_dir = Path(data_dir) self.data_dir.mkdir(parents=True, exist_ok=True) print("📁 Student Feedback Extractor initialized") # Sample data definitions self.students = [f"STU{i:04d}" for i in range(1, 201)] # 200 students self.courses = [ "MATH101", "MATH201", "MATH301", "ENG101", "ENG201", "ENG301", "SCI101", "SCI201", "SCI301", "HIST101", "HIST201", "PHIL101", "ART101", "MUS101", "CHEM101", "PHYS101", "BIO101", "COMP101" ] self.instructors = [ "PROF001", "PROF002", "PROF003", "PROF004", "PROF005", "PROF006", "PROF007", "PROF008", "PROF009", "PROF010" ] self.semesters = ["Fall2023", "Spring2024", "Summer2024", "Fall2024"] def generate_realistic_ratings(self): """Generate more realistic rating patterns""" # Most ratings tend to be 3-5, with 4 being most common weights = [5, 10, 25, 45, 15] # for ratings 1,2,3,4,5 return random.choices([1, 2, 3, 4, 5], weights=weights)[0] def create_comprehensive_sample_data(self, num_records=500): """Create comprehensive sample student feedback data""" print(f"🔄 Generating {num_records} realistic feedback records...") records = [] feedback_id = 1 for _ in range(num_records): # Basic information student_id = random.choice(self.students) course_id = random.choice(self.courses) instructor_id = random.choice(self.instructors) semester = random.choice(self.semesters) # Generate correlated ratings (good instructors tend to get good ratings across categories) base_quality = random.uniform(2.5, 4.5) # Instructor's "true" quality overall_rating = max(1, min(5, int(base_quality + random.uniform(-0.5, 0.5)))) course_content_rating = max(1, min(5, int(base_quality + random.uniform(-0.7, 0.7)))) instructor_effectiveness = max(1, min(5, int(base_quality + random.uniform(-0.3, 0.3)))) # These can vary more independently difficulty_level = random.randint(1, 5) workload_rating = random.randint(1, 5) recommendation_score = max(1, min(5, int(base_quality + random.uniform(-1, 1)))) # Attendance and assignment quality attendance_rate = random.uniform(0.65, 1.0) assignment_quality = max(1, min(5, int(base_quality + random.uniform(-0.8, 0.8)))) # Generate feedback date start_date = datetime.now() - timedelta(days=365) random_days = random.randint(0, 365) feedback_date = start_date + timedelta(days=random_days) record = { 'feedback_id': feedback_id, 'student_id': student_id, 'course_id': course_id, 'instructor_id': instructor_id, 'semester': semester, 'overall_rating': overall_rating, 'course_content_rating': course_content_rating, 'instructor_effectiveness': instructor_effectiveness, 'difficulty_level': difficulty_level, 'workload_rating': workload_rating, 'recommendation_score': recommendation_score, 'attendance_rate': round(attendance_rate, 2), 'assignment_quality': assignment_quality, 'feedback_date': feedback_date.strftime('%Y-%m-%d'), 'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S') } records.append(record) feedback_id += 1 return records def save_to_csv(self, records, filename="student_feedback.csv"): """Save records to CSV file""" file_path = self.data_dir / filename if records: fieldnames = list(records[0].keys()) with open(file_path, 'w', newline='', encoding='utf-8') as csvfile: writer = csv.DictWriter(csvfile, fieldnames=fieldnames) writer.writeheader() writer.writerows(records) print(f"💾 Saved {len(records)} records to {file_path}") return file_path def save_metadata(self, records, filename="metadata.json"): """Save metadata about the dataset""" file_path = self.data_dir / filename metadata = { 'total_records': len(records), 'date_generated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 'unique_students': len(set(r['student_id'] for r in records)), 'unique_courses': len(set(r['course_id'] for r in records)), 'unique_instructors': len(set(r['instructor_id'] for r in records)), 'semesters_covered': list(set(r['semester'] for r in records)), 'rating_distribution': self._get_rating_distribution(records), 'data_description': { 'student_id': 'Unique identifier for each student', 'course_id': 'Course code identifier', 'instructor_id': 'Instructor identifier', 'semester': 'Academic semester', 'overall_rating': 'Overall course rating (1-5)', 'course_content_rating': 'Course content quality (1-5)', 'instructor_effectiveness': 'Instructor teaching effectiveness (1-5)', 'difficulty_level': 'Perceived course difficulty (1-5)', 'workload_rating': 'Course workload rating (1-5)', 'recommendation_score': 'Likelihood to recommend (1-5)', 'attendance_rate': 'Student attendance rate (0.0-1.0)', 'assignment_quality': 'Quality of assignments (1-5)' } } with open(file_path, 'w', encoding='utf-8') as jsonfile: json.dump(metadata, jsonfile, indent=2) print(f"📊 Metadata saved to {file_path}") return metadata def _get_rating_distribution(self, records): """Calculate rating distribution for metadata""" distribution = {} rating_fields = ['overall_rating', 'course_content_rating', 'instructor_effectiveness'] for field in rating_fields: dist = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0} for record in records: rating = record[field] dist[rating] += 1 distribution[field] = dist return distribution def extract_data(self, num_records=500): """Main extraction method""" print("🚀 Starting data extraction process...") # Generate sample data records = self.create_comprehensive_sample_data(num_records) # Save to CSV csv_path = self.save_to_csv(records) # Save metadata metadata = self.save_metadata(records) print("✅ Data extraction completed successfully!") return { 'records': records, 'csv_path': csv_path, 'metadata': metadata } def load_existing_data(self, filename="student_feedback.csv"): """Load existing data from CSV file""" file_path = self.data_dir / filename if not file_path.exists(): print(f"❌ File {file_path} not found") return None records = [] with open(file_path, 'r', encoding='utf-8') as csvfile: reader = csv.DictReader(csvfile) for row in reader: # Convert numeric fields back to appropriate types row['feedback_id'] = int(row['feedback_id']) row['overall_rating'] = int(row['overall_rating']) row['course_content_rating'] = int(row['course_content_rating']) row['instructor_effectiveness'] = int(row['instructor_effectiveness']) row['difficulty_level'] = int(row['difficulty_level']) row['workload_rating'] = int(row['workload_rating']) row['recommendation_score'] = int(row['recommendation_score']) row['attendance_rate'] = float(row['attendance_rate']) row['assignment_quality'] = int(row['assignment_quality']) records.append(row) print(f"📖 Loaded {len(records)} records from {file_path}") return records if __name__ == "__main__": # Test the extractor extractor = StudentFeedbackExtractor() # Extract data result = extractor.extract_data(num_records=300) # Show sample records print("\n�� Sample records:") for i, record in enumerate(result['records'][:3]): print(f"\nRecord {i+1}:") for key, value in record.items(): print(f" {key}: {value}") print(f"\n📈 Dataset summary:") print(f"Total records: {result['metadata']['total_records']}") print(f"Unique students: {result['metadata']['unique_students']}") print(f"Unique courses: {result['metadata']['unique_courses']}") print(f"Unique instructors: {result['metadata']['unique_instructors']}") EOF
python3 src/extract/data_extractor.py
cat > src/transform/data_transformer.py <<'EOF' """ Student Feedback Data Transformer ================================ This module handles data cleaning, validation, and feature engineering. """ import csv import json import statistics from datetime import datetime from pathlib import Path from collections import defaultdict class StudentFeedbackTransformer: """Transform and enhance student feedback data""" def __init__(self, input_dir="data/raw", output_dir="data/processed"): self.input_dir = Path(input_dir) self.output_dir = Path(output_dir) self.output_dir.mkdir(parents=True, exist_ok=True) print("🔄 Student Feedback Transformer initialized") # Data quality tracking self.quality_report = { 'records_processed': 0, 'records_cleaned': 0, 'missing_values_filled': 0, 'outliers_detected': 0, 'features_created': 0 } def load_raw_data(self, filename="student_feedback.csv"): """Load raw data from CSV""" file_path = self.input_dir / filename if not file_path.exists(): raise FileNotFoundError(f"Raw data file not found: {file_path}") records = [] with open(file_path, 'r', encoding='utf-8') as csvfile: reader = csv.DictReader(csvfile) for row in reader: records.append(row) print(f"📖 Loaded {len(records)} raw records") self.quality_report['records_processed'] = len(records) return records def validate_and_clean_data(self, records): """Validate and clean the raw data""" print("🧹 Starting data validation and cleaning...") cleaned_records = [] for record in records: # Convert string numbers to appropriate types try: record['feedback_id'] = int(record['feedback_id']) record['overall_rating'] = int(record['overall_rating']) record['course_content_rating'] = int(record['course_content_rating']) record['instructor_effectiveness'] = int(record['instructor_effectiveness']) record['difficulty_level'] = int(record['difficulty_level']) record['workload_rating'] = int(record['workload_rating']) record['recommendation_score'] = int(record['recommendation_score']) record['attendance_rate'] = float(record['attendance_rate']) record['assignment_quality'] = int(record['assignment_quality']) except (ValueError, KeyError) as e: print(f"⚠️ Skipping invalid record: {e}") continue # Validate rating ranges (should be 1-5) rating_fields = ['overall_rating', 'course_content_rating', 'instructor_effectiveness', 'difficulty_level', 'workload_rating', 'recommendation_score', 'assignment_quality'] valid_record = True for field in rating_fields: if record[field] < 1 or record[field] > 5: print(f"⚠️ Invalid {field}: {record[field]} for record {record['feedback_id']}") # Clip to valid range record[field] = max(1, min(5, record[field])) self.quality_report['outliers_detected'] += 1 # Validate attendance rate (should be 0.0-1.0) if record['attendance_rate'] < 0 or record['attendance_rate'] > 1: print(f"⚠️ Invalid attendance_rate: {record['attendance_rate']}") record['attendance_rate'] = max(0.0, min(1.0, record['attendance_rate'])) self.quality_report['outliers_detected'] += 1 # Check for required fields required_fields = ['student_id', 'course_id', 'instructor_id', 'semester'] if all(record.get(field) for field in required_fields): cleaned_records.append(record) self.quality_report['records_cleaned'] += 1 else: print(f"⚠️ Skipping record with missing required fields: {record['feedback_id']}") print(f"✅ Cleaned {len(cleaned_records)} records") return cleaned_records def create_derived_features(self, records): """Create new features from existing data""" print("➕ Creating derived features...") enhanced_records = [] for record in records: # Create satisfaction score (weighted average of key ratings) satisfaction_score = ( record['overall_rating'] config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 0.3 + record['course_content_rating'] config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 0.25 + record['instructor_effectiveness'] config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 0.25 + record['recommendation_score'] config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 0.2 ) record['satisfaction_score'] = round(satisfaction_score, 2) # Create engagement score engagement_score = ( record['attendance_rate'] config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 0.4 + (record['assignment_quality'] / 5) config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 0.6 ) record['engagement_score'] = round(engagement_score, 2) # Create difficulty categories difficulty_level = record['difficulty_level'] if difficulty_level <= 2: record['difficulty_category'] = 'Easy' elif difficulty_level == 3: record['difficulty_category'] = 'Moderate' elif difficulty_level == 4: record['difficulty_category'] = 'Hard' else: record['difficulty_category'] = 'Very Hard' # Create performance categories based on satisfaction satisfaction = record['satisfaction_score'] if satisfaction >= 4.0: record['performance_category'] = 'Excellent' elif satisfaction >= 3.5: record['performance_category'] = 'Good' elif satisfaction >= 2.5: record['performance_category'] = 'Fair' else: record['performance_category'] = 'Poor' # Create semester season and year semester = record['semester'] if 'Fall' in semester: record['semester_season'] = 'Fall' elif 'Spring' in semester: record['semester_season'] = 'Spring' elif 'Summer' in semester: record['semester_season'] = 'Summer' else: record['semester_season'] = 'Unknown' # Extract year try: record['semester_year'] = int(''.join(filter(str.isdigit, semester))) except ValueError: record['semester_year'] = 2024 # Default year # Create course department (first 3-4 letters of course_id) course_id = record['course_id'] record['department'] = ''.join(filter(str.isalpha, course_id)) # Create course level based on course number course_number = ''.join(filter(str.isdigit, course_id)) try: level = int(course_number) if level < 200: record['course_level'] = 'Introductory' elif level < 300: record['course_level'] = 'Intermediate' else: record['course_level'] = 'Advanced' except ValueError: record['course_level'] = 'Unknown' enhanced_records.append(record) self.quality_report['features_created'] += 1 print(f"✅ Enhanced {len(enhanced_records)} records with derived features") return enhanced_records def calculate_aggregations(self, records): """Calculate useful aggregations""" print("📊 Calculating aggregations...") # Group by instructor instructor_stats = defaultdict(lambda: { 'total_ratings': 0, 'total_satisfaction': 0, 'total_engagement': 0, 'courses_taught': set(), 'semesters_active': set(), 'ratings': [] }) # Group by course course_stats = defaultdict(lambda: { 'total_ratings': 0, 'total_satisfaction': 0, 'total_difficulty': 0, 'instructors': set(), 'semesters_offered': set(), 'ratings': [] }) # Group by semester semester_stats = defaultdict(lambda: { 'total_ratings': 0, 'total_satisfaction': 0, 'courses_offered': set(), 'instructors_active': set() }) for record in records: instructor_id = record['instructor_id'] course_id = record['course_id'] semester = record['semester'] satisfaction = record['satisfaction_score'] engagement = record['engagement_score'] overall_rating = record['overall_rating'] # Instructor stats instructor_stats[instructor_id]['total_ratings'] += 1 instructor_stats[instructor_id]['total_satisfaction'] += satisfaction instructor_stats[instructor_id]['total_engagement'] += engagement instructor_stats[instructor_id]['courses_taught'].add(course_id) instructor_stats[instructor_id]['semesters_active'].add(semester) instructor_stats[instructor_id]['ratings'].append(overall_rating) # Course stats course_stats[course_id]['total_ratings'] += 1 course_stats[course_id]['total_satisfaction'] += satisfaction course_stats[course_id]['total_difficulty'] += record['difficulty_level'] course_stats[course_id]['instructors'].add(instructor_id) course_stats[course_id]['semesters_offered'].add(semester) course_stats[course_id]['ratings'].append(overall_rating) # Semester stats semester_stats[semester]['total_ratings'] += 1 semester_stats[semester]['total_satisfaction'] += satisfaction semester_stats[semester]['courses_offered'].add(course_id) semester_stats[semester]['instructors_active'].add(instructor_id) # Calculate averages and additional metrics aggregations = { 'instructors': {}, 'courses': {}, 'semesters': {} } # Process instructor stats for instructor_id, stats in instructor_stats.items(): if stats['total_ratings'] > 0: aggregations['instructors'][instructor_id] = { 'total_reviews': stats['total_ratings'], 'avg_satisfaction': round(stats['total_satisfaction'] / stats['total_ratings'], 2), 'avg_engagement': round(stats['total_engagement'] / stats['total_ratings'], 2), 'courses_taught': len(stats['courses_taught']), 'semesters_active': len(stats['semesters_active']), 'rating_std': round(statistics.stdev(stats['ratings']) if len(stats['ratings']) > 1 else 0, 2), 'min_rating': min(stats['ratings']), 'max_rating': max(stats['ratings']) } # Process course stats for course_id, stats in course_stats.items(): if stats['total_ratings'] > 0: aggregations['courses'][course_id] = { 'total_reviews': stats['total_ratings'], 'avg_satisfaction': round(stats['total_satisfaction'] / stats['total_ratings'], 2), 'avg_difficulty': round(stats['total_difficulty'] / stats['total_ratings'], 2), 'instructors_count': len(stats['instructors']), 'semesters_offered': len(stats['semesters_offered']), 'rating_std': round(statistics.stdev(stats['ratings']) if len(stats['ratings']) > 1 else 0, 2) } # Process semester stats for semester, stats in semester_stats.items(): if stats['total_ratings'] > 0: aggregations['semesters'][semester] = { 'total_reviews': stats['total_ratings'], 'avg_satisfaction': round(stats['total_satisfaction'] / stats['total_ratings'], 2), 'courses_offered': len(stats['courses_offered']), 'instructors_active': len(stats['instructors_active']) } return aggregations def save_processed_data(self, records, filename="processed_feedback.csv"): """Save processed data to CSV""" file_path = self.output_dir / filename if records: # Get all possible fieldnames all_fields = set() for record in records: all_fields.update(record.keys()) fieldnames = sorted(list(all_fields)) with open(file_path, 'w', newline='', encoding='utf-8') as csvfile: writer = csv.DictWriter(csvfile, fieldnames=fieldnames) writer.writeheader() writer.writerows(records) print(f"💾 Saved {len(records)} processed records to {file_path}") return file_path def save_aggregations(self, aggregations, filename="aggregations.json"): """Save aggregated statistics""" file_path = self.output_dir / filename with open(file_path, 'w', encoding='utf-8') as jsonfile: json.dump(aggregations, jsonfile, indent=2) print(f"📊 Saved aggregations to {file_path}") return file_path def save_quality_report(self, filename="data_quality_report.json"): """Save data quality report""" file_path = self.output_dir / filename self.quality_report['transformation_date'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S') self.quality_report['data_quality_score'] = round( (self.quality_report['records_cleaned'] / self.quality_report['records_processed']) config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 100, 2 ) if self.quality_report['records_processed'] > 0 else 0 with open(file_path, 'w', encoding='utf-8') as jsonfile: json.dump(self.quality_report, jsonfile, indent=2) print(f"📋 Quality report saved to {file_path}") return self.quality_report def transform(self, input_filename="student_feedback.csv"): """Main transformation pipeline""" print("🚀 Starting data transformation pipeline...") # Load raw data raw_records = self.load_raw_data(input_filename) # Clean and validate cleaned_records = self.validate_and_clean_data(raw_records) # Create derived features enhanced_records = self.create_derived_features(cleaned_records) # Calculate aggregations aggregations = self.calculate_aggregations(enhanced_records) # Save results processed_file = self.save_processed_data(enhanced_records) aggregations_file = self.save_aggregations(aggregations) quality_report = self.save_quality_report() print("✅ Data transformation completed successfully!") return { 'processed_records': enhanced_records, 'aggregations': aggregations, 'quality_report': quality_report, 'files': { 'processed_data': processed_file, 'aggregations': aggregations_file } } if __name__ == "__main__": # Test the transformer transformer = StudentFeedbackTransformer() # Transform the data result = transformer.transform() # Show results print(f"\n📊 Transformation Summary:") print(f"Records processed: {result['quality_report']['records_processed']}") print(f"Records cleaned: {result['quality_report']['records_cleaned']}") print(f"Data quality score: {result['quality_report']['data_quality_score']}%") print(f"Features created: {result['quality_report']['features_created']}") # Show sample enhanced record if result['processed_records']: print(f"\n📋 Sample enhanced record:") sample = result['processed_records'][0] for key, value in sample.items(): print(f" {key}: {value}") # Show aggregation summary print(f"\n📈 Aggregation Summary:") print(f"Instructors analyzed: {len(result['aggregations']['instructors'])}") print(f"Courses analyzed: {len(result['aggregations']['courses'])}") print(f"Semesters analyzed: {len(result['aggregations']['semesters'])}") EOF
ls --color=auto -la data/processed/
python3 src/extract/data_extractor.py
ls --color=auto -la data/raw/
python3 src/transform/data_transformer.py
cat > scripts/run_complete_etl.py <<'EOF' """ Complete Academic Pulse ETL Pipeline =================================== This script runs the entire ETL process in the correct order. """ import sys import os from pathlib import Path # Add src to Python path sys.path.append('src') def run_extraction(): """Run data extraction""" print("=" config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 60) print("📥 STEP 1: DATA EXTRACTION") print("=" config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 60) try: from extract.data_extractor import StudentFeedbackExtractor extractor = StudentFeedbackExtractor() result = extractor.extract_data(num_records=500) print(f"✅ Extraction completed successfully!") print(f" - Records created: {len(result['records'])}") print(f" - File saved: {result['csv_path']}") return True, result except Exception as e: print(f"❌ Extraction failed: {e}") return False, None def run_transformation(): """Run data transformation""" print("\n" + "=" config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 60) print("🔄 STEP 2: DATA TRANSFORMATION") print("=" config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 60) try: from transform.data_transformer import StudentFeedbackTransformer transformer = StudentFeedbackTransformer() result = transformer.transform() print(f"✅ Transformation completed successfully!") print(f" - Records processed: {result['quality_report']['records_processed']}") print(f" - Records cleaned: {result['quality_report']['records_cleaned']}") print(f" - Data quality score: {result['quality_report']['data_quality_score']}%") return True, result except Exception as e: print(f"❌ Transformation failed: {e}") return False, None def run_analysis(): """Run data analysis""" print("\n" + "=" config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 60) print("📊 STEP 3: DATA ANALYSIS") print("=" config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 60) try: # Load processed data and run basic analysis processed_file = Path("data/processed/processed_feedback.csv") if not processed_file.exists(): print("❌ No processed data file found") return False, None import csv from collections import defaultdict, Counter # Load data records = [] with open(processed_file, 'r', encoding='utf-8') as csvfile: reader = csv.DictReader(csvfile) for row in reader: records.append(row) print(f"📖 Loaded {len(records)} processed records") # Basic analysis total_records = len(records) # Convert numeric fields for record in records: record['satisfaction_score'] = float(record['satisfaction_score']) record['engagement_score'] = float(record['engagement_score']) record['overall_rating'] = int(record['overall_rating']) # Calculate averages avg_satisfaction = sum(r['satisfaction_score'] for r in records) / total_records avg_engagement = sum(r['engagement_score'] for r in records) / total_records avg_overall = sum(r['overall_rating'] for r in records) / total_records print(f"\n📈 OVERALL STATISTICS:") print(f" Average Satisfaction Score: {avg_satisfaction:.2f}/5") print(f" Average Engagement Score: {avg_engagement:.2f}/5") print(f" Average Overall Rating: {avg_overall:.2f}/5") # Performance distribution performance_dist = Counter(r['performance_category'] for r in records) print(f"\n🏆 PERFORMANCE DISTRIBUTION:") for category, count in performance_dist.items(): percentage = (count / total_records) config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 100 print(f" {category}: {count} ({percentage:.1f}%)") # Top instructors instructor_scores = defaultdict(list) for record in records: instructor_scores[record['instructor_id']].append(record['satisfaction_score']) instructor_averages = { instructor: sum(scores) / len(scores) for instructor, scores in instructor_scores.items() } top_instructors = sorted(instructor_averages.items(), key=lambda x: x[1], reverse=True)[:5] print(f"\n🥇 TOP 5 INSTRUCTORS:") for i, (instructor, score) in enumerate(top_instructors, 1): print(f" {i}. {instructor}: {score:.2f}/5") # Course analysis course_scores = defaultdict(list) for record in records: course_scores[record['course_id']].append(record['satisfaction_score']) course_averages = { course: sum(scores) / len(scores) for course, scores in course_scores.items() } top_courses = sorted(course_averages.items(), key=lambda x: x[1], reverse=True)[:5] print(f"\n📚 TOP 5 COURSES:") for i, (course, score) in enumerate(top_courses, 1): print(f" {i}. {course}: {score:.2f}/5") # Difficulty analysis difficulty_dist = Counter(r['difficulty_category'] for r in records) print(f"\n⚡ DIFFICULTY DISTRIBUTION:") for category, count in difficulty_dist.items(): percentage = (count / total_records) config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 100 print(f" {category}: {count} ({percentage:.1f}%)") # Department analysis dept_scores = defaultdict(list) for record in records: dept_scores[record['department']].append(record['satisfaction_score']) dept_averages = { dept: sum(scores) / len(scores) for dept, scores in dept_scores.items() } print(f"\n🏢 DEPARTMENT PERFORMANCE:") for dept, score in sorted(dept_averages.items(), key=lambda x: x[1], reverse=True): count = len(dept_scores[dept]) print(f" {dept}: {score:.2f}/5 (n={count})") return True, { 'total_records': total_records, 'avg_satisfaction': avg_satisfaction, 'avg_engagement': avg_engagement, 'performance_dist': dict(performance_dist), 'top_instructors': top_instructors[:3], 'top_courses': top_courses[:3] } except Exception as e: print(f"❌ Analysis failed: {e}") return False, None def main(): """Run complete ETL pipeline""" print("🎓 ACADEMIC PULSE ETL PIPELINE") print("🚀 Starting complete ETL process...") # Step 1: Extract extract_success, extract_result = run_extraction() if not extract_success: print("❌ Pipeline failed at extraction step") return # Step 2: Transform transform_success, transform_result = run_transformation() if not transform_success: print("❌ Pipeline failed at transformation step") return # Step 3: Analyze analyze_success, analyze_result = run_analysis() if not analyze_success: print("❌ Pipeline failed at analysis step") return # Final summary print("\n" + "=" config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 60) print("🎉 ETL PIPELINE COMPLETED SUCCESSFULLY!") print("=" config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 60) print(f"\n📊 FINAL SUMMARY:") print(f" Raw records extracted: {len(extract_result['records'])}") print(f" Records processed: {transform_result['quality_report']['records_cleaned']}") print(f" Data quality score: {transform_result['quality_report']['data_quality_score']}%") print(f" Average satisfaction: {analyze_result['avg_satisfaction']:.2f}/5") print(f"\n📁 FILES CREATED:") print(f" - Raw data: data/raw/student_feedback.csv") print(f" - Processed data: data/processed/processed_feedback.csv") print(f" - Metadata: data/raw/metadata.json") print(f" - Quality report: data/processed/data_quality_report.json") print(f" - Aggregations: data/processed/aggregations.json") print(f"\n🏆 TOP PERFORMERS:") for i, (instructor, score) in enumerate(analyze_result['top_instructors'], 1): print(f" {i}. Instructor {instructor}: {score:.2f}/5") print(f"\n📚 TOP COURSES:") for i, (course, score) in enumerate(analyze_result['top_courses'], 1): print(f" {i}. {course}: {score:.2f}/5") if __name__ == "__main__": main() EOF
mkdir -p scripts
echo "📁 Checking all created files..."
cat > scripts/data_summary.py <<'EOF' """ Quick Data Summary Script ======================== Provides a quick overview of the processed data. """ import csv import json from pathlib import Path from collections import Counter def load_processed_data(): """Load processed data""" file_path = Path("data/processed/processed_feedback.csv") if not file_path.exists(): print("❌ No processed data found. Run the ETL pipeline first!") return None records = [] with open(file_path, 'r', encoding='utf-8') as csvfile: reader = csv.DictReader(csvfile) for row in reader: # Convert numeric fields row['satisfaction_score'] = float(row['satisfaction_score']) row['engagement_score'] = float(row['engagement_score']) row['overall_rating'] = int(row['overall_rating']) records.append(row) return records def print_summary(records): """Print data summary""" if not records: return total = len(records) print("📊 ACADEMIC PULSE DATA SUMMARY") print("=" config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 50) print(f"Total Feedback Records: {total}") # Basic stats avg_satisfaction = sum(r['satisfaction_score'] for r in records) / total avg_engagement = sum(r['engagement_score'] for r in records) / total avg_overall = sum(r['overall_rating'] for r in records) / total print(f"\n📈 Average Scores:") print(f" Satisfaction: {avg_satisfaction:.2f}/5") print(f" Engagement: {avg_engagement:.2f}/5") print(f" Overall Rating: {avg_overall:.2f}/5") # Unique counts unique_students = len(set(r['student_id'] for r in records)) unique_courses = len(set(r['course_id'] for r in records)) unique_instructors = len(set(r['instructor_id'] for r in records)) unique_semesters = len(set(r['semester'] for r in records)) print(f"\n📋 Data Coverage:") print(f" Students: {unique_students}") print(f" Courses: {unique_courses}") print(f" Instructors: {unique_instructors}") print(f" Semesters: {unique_semesters}") # Performance distribution performance_dist = Counter(r['performance_category'] for r in records) print(f"\n🏆 Performance Distribution:") for category in ['Excellent', 'Good', 'Fair', 'Poor']: if category in performance_dist: count = performance_dist[category] pct = (count / total) config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 100 print(f" {category}: {count} ({pct:.1f}%)") # Difficulty distribution difficulty_dist = Counter(r['difficulty_category'] for r in records) print(f"\n⚡ Difficulty Distribution:") for category in ['Easy', 'Moderate', 'Hard', 'Very Hard']: if category in difficulty_dist: count = difficulty_dist[category] pct = (count / total) config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 100 print(f" {category}: {count} ({pct:.1f}%)") def main(): records = load_processed_data() if records: print_summary(records) else: print("Run: python3 scripts/run_complete_etl.py") if __name__ == "__main__": main() EOF
python3 scripts/data_summary.py
cat > scripts/create_basic_charts.py <<'EOF' """ Basic Charts for Academic Pulse Data ==================================== Creates simple text-based charts and basic analysis. """ import csv from pathlib import Path from collections import Counter, defaultdict def load_data(): """Load processed data""" file_path = Path("data/processed/processed_feedback.csv") if not file_path.exists(): print("❌ No processed data found.") return None records = [] with open(file_path, 'r', encoding='utf-8') as csvfile: reader = csv.DictReader(csvfile) for row in reader: row['satisfaction_score'] = float(row['satisfaction_score']) row['overall_rating'] = int(row['overall_rating']) records.append(row) return records def create_text_chart(data, title, max_width=50): """Create a simple text-based bar chart""" print(f"\n📊 {title}") print("=" config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations len(title)) if not data: return max_value = max(data.values()) for label, value in data.items(): bar_length = int((value / max_value) config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations max_width) bar = "█" config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations bar_length print(f"{label:15} │{bar:<{max_width}} │ {value}") def analyze_performance_by_category(records): """Analyze performance by different categories""" # Performance distribution performance_dist = Counter(r['performance_category'] for r in records) create_text_chart(dict(performance_dist), "Performance Distribution") # Difficulty distribution difficulty_dist = Counter(r['difficulty_category'] for r in records) create_text_chart(dict(difficulty_dist), "Difficulty Distribution") # Department performance dept_scores = defaultdict(list) for record in records: dept_scores[record['department']].append(record['satisfaction_score']) dept_averages = { dept: round(sum(scores) / len(scores), 2) for dept, scores in dept_scores.items() } create_text_chart(dept_averages, "Average Satisfaction by Department") # Semester trends semester_scores = defaultdict(list) for record in records: semester_scores[record['semester']].append(record['satisfaction_score']) semester_averages = { semester: round(sum(scores) / len(scores), 2) for semester, scores in semester_scores.items() } create_text_chart(semester_averages, "Average Satisfaction by Semester") def create_top_performers_chart(records): """Show top performing instructors and courses""" # Top instructors instructor_scores = defaultdict(list) for record in records: instructor_scores[record['instructor_id']].append(record['satisfaction_score']) instructor_averages = { instructor: sum(scores) / len(scores) for instructor, scores in instructor_scores.items() if len(scores) >= 5 # At least 5 reviews } top_instructors = dict(sorted(instructor_averages.items(), key=lambda x: x[1], reverse=True)[:8]) top_instructors = {k: round(v, 2) for k, v in top_instructors.items()} create_text_chart(top_instructors, "Top Instructors (≥5 reviews)") # Top courses course_scores = defaultdict(list) for record in records: course_scores[record['course_id']].append(record['satisfaction_score']) course_averages = { course: sum(scores) / len(scores) for course, scores in course_scores.items() if len(scores) >= 3 # At least 3 reviews } top_courses = dict(sorted(course_averages.items(), key=lambda x: x[1], reverse=True)[:8]) top_courses = {k: round(v, 2) for k, v in top_courses.items()} create_text_chart(top_courses, "Top Courses (≥3 reviews)") def create_correlation_analysis(records): """Analyze correlations between different metrics""" print(f"\n📈 CORRELATION ANALYSIS") print("=" config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 25) # Difficulty vs Satisfaction easy_courses = [r for r in records if r['difficulty_category'] == 'Easy'] hard_courses = [r for r in records if r['difficulty_category'] == 'Very Hard'] if easy_courses and hard_courses: easy_avg = sum(r['satisfaction_score'] for r in easy_courses) / len(easy_courses) hard_avg = sum(r['satisfaction_score'] for r in hard_courses) / len(hard_courses) print(f"Easy courses satisfaction: {easy_avg:.2f}/5 (n={len(easy_courses)})") print(f"Very hard courses satisfaction: {hard_avg:.2f}/5 (n={len(hard_courses)})") print(f"Difference: {easy_avg - hard_avg:.2f} points") # Engagement vs Satisfaction high_engagement = [r for r in records if r['engagement_score'] > '0.8'] low_engagement = [r for r in records if r['engagement_score'] < '0.6'] if high_engagement and low_engagement: high_sat = sum(r['satisfaction_score'] for r in high_engagement) / len(high_engagement) low_sat = sum(r['satisfaction_score'] for r in low_engagement) / len(low_engagement) print(f"\nHigh engagement satisfaction: {high_sat:.2f}/5 (n={len(high_engagement)})") print(f"Low engagement satisfaction: {low_sat:.2f}/5 (n={len(low_engagement)})") print(f"Difference: {high_sat - low_sat:.2f} points") def main(): """Main function""" print("📊 ACADEMIC PULSE - DATA VISUALIZATION") print("=" config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 45) records = load_data() if not records: print("Run the ETL pipeline first: python3 scripts/run_complete_etl.py") return print(f"📖 Analyzing {len(records)} feedback records...") # Create various analyses analyze_performance_by_category(records) create_top_performers_chart(records) create_correlation_analysis(records) print(f"\n🎉 Analysis complete!") print(f"💡 Insights:") print(f" - Review the top performers for best practices") print(f" - Focus improvement efforts on poor-performing areas") print(f" - Consider difficulty-satisfaction relationships for course design") if __name__ == "__main__": main() EOF
python3 scripts/create_basic_charts.py
cat > README.md <<'EOF' # 🎓 Academic Pulse ETL & Analytics Project A comprehensive ETL (Extract, Transform, Load) pipeline for analyzing student feedback data to provide actionable insights for academic institutions. ## 📋 Project Overview This project demonstrates a complete data engineering workflow: - **Extract**: Generate realistic student feedback data - **Transform**: Clean, validate, and enhance the data - **Load**: Store processed data with proper structure - **Analyze**: Generate insights and visualizations ## 🚀 Quick Start Run the complete ETL pipeline: ```bash python3 scripts/run_complete_etl.py python3 scripts/data_summary.py python3 scripts/data_summary.py python3 scripts/create_basic_charts.py ### 6.2 Create Final Project Summary Script ```bash cat > scripts/project_summary.py << 'EOF' """ Academic Pulse Project Summary ============================= Provides a complete overview of the project and its capabilities. """ import os from pathlib import Path def check_project_structure(): """Check if all project files exist""" print("📁 PROJECT STRUCTURE CHECK") print("=" config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 30) required_files = [ "src/extract/data_extractor.py", "src/transform/data_transformer.py", "scripts/run_complete_etl.py", "scripts/data_summary.py", "scripts/create_basic_charts.py", "README.md" ] required_dirs = [ "data/raw", "data/processed", "src/extract", "src/transform", "scripts" ] print("Files:") for file_path in required_files: exists = "✅" if Path(file_path).exists() else "❌" print(f" {exists} {file_path}") print("\nDirectories:") for dir_path in required_dirs: exists = "✅" if Path(dir_path).exists() else "❌" print(f" {exists} {dir_path}") def check_data_files(): """Check if data files have been generated""" print(f"\n📊 DATA FILES CHECK") print("=" config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 25) data_files = [ "data/raw/student_feedback.csv", "data/raw/metadata.json", "data/processed/processed_feedback.csv", "data/processed/data_quality_report.json", "data/processed/aggregations.json" ] for file_path in data_files: path = Path(file_path) if path.exists(): size = path.stat().st_size print(f" ✅ {file_path} ({size:,} bytes)") else: print(f" ❌ {file_path} (missing)") def show_available_commands(): """Show available commands""" print(f"\n🚀 AVAILABLE COMMANDS") print("=" config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 25) commands = [ ("python3 scripts/run_complete_etl.py", "Run full ETL pipeline"), ("python3 scripts/data_summary.py", "Show data summary"), ("python3 scripts/create_basic_charts.py", "Create visualizations"), ("python3 src/extract/data_extractor.py", "Extract data only"), ("python3 src/transform/data_transformer.py", "Transform data only") ] for command, description in commands: print(f" 📌 {command}") print(f" {description}") print() def show_project_capabilities(): """Show what the project can do""" print(f"🎯 PROJECT CAPABILITIES") print("=" config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 30) capabilities = [ "📥 Extract realistic student feedback data (500+ records)", "🔄 Transform and clean data with quality validation", "📊 Generate comprehensive analytics and insights", "🏆 Identify top-performing instructors and courses", "📈 Analyze trends across semesters and departments", "⚡ Study difficulty-satisfaction correlations", "📋 Track data quality and processing metrics", "📊 Create text-based data visualizations", "💾 Export data in structured CSV format", "📝 Generate detailed metadata and reports" ] for capability in capabilities: print(f" {capability}") def main(): """Main summary function""" print("🎓 ACADEMIC PULSE ETL PROJECT") print("🔍 Complete Project Summary") print("=" config data freeCodeCamp my_first_script.py notebooks reports scripts src tests visualizations 50) check_project_structure() check_data_files() show_available_commands() show_project_capabilities() print(f"\n🎉 PROJECT SUMMARY COMPLETE!") print(f"💡 Next steps:") print(f" 1. Run the ETL pipeline if you haven't already") print(f" 2. Explore the generated insights") print(f" 3. Experiment with the analysis scripts") print(f" 4. Consider extending with new features") if __name__ == "__main__": main() EOF
python3 scripts/project_summary.py
cd /workspace/project
ls --color=auto -la
python3 src/extract/data_extractor.py
cat > test_setup.py <<'EOF' import os import sys from pathlib import Path print("🔍 DIAGNOSTIC CHECK") print("=" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests visualizations 30) # Check current directory print(f"Current directory: {os.getcwd()}") # Check Python path print(f"Python path: {sys.path}") # Check if directories exist dirs_to_check = [ "src", "src/extract", "src/transform", "data", "data/raw", "data/processed", "scripts" ] print(f"\n📁 Directory Check:") for dir_name in dirs_to_check: exists = "✅" if Path(dir_name).exists() else "❌" print(f" {exists} {dir_name}") # Check if Python files exist files_to_check = [ "src/extract/data_extractor.py", "src/transform/data_transformer.py" ] print(f"\n📄 File Check:") for file_name in files_to_check: exists = "✅" if Path(file_name).exists() else "❌" print(f" {exists} {file_name}") # Test basic import print(f"\n🐍 Testing Python imports:") try: import csv print(" ✅ csv module") except ImportError: print(" ❌ csv module") try: import json print(" ✅ json module") except ImportError: print(" ❌ json module") try: from pathlib import Path print(" ✅ pathlib module") except ImportError: print(" ❌ pathlib module") print(f"\n🚀 Ready to proceed!") EOF
python3 test_setup.py
mkdir -p src/extract src/transform src/load src/analyze
cat > src/extract/data_extractor.py <<'EOF' import csv import random import json from datetime import datetime from pathlib import Path class StudentFeedbackExtractor: def __init__(self): self.data_dir = Path("data/raw") self.data_dir.mkdir(parents=True, exist_ok=True) print("📁 Data extractor initialized") def create_sample_data(self, num_records=100): print(f"🔄 Creating {num_records} sample records...") records = [] for i in range(1, num_records + 1): record = { 'feedback_id': i, 'student_id': f'STU{i:03d}', 'course_id': f'COURSE{random.randint(1, 10):02d}', 'instructor_id': f'INST{random.randint(1, 5):02d}', 'semester': random.choice(['Fall2024', 'Spring2024', 'Summer2024']), 'overall_rating': random.randint(1, 5), 'course_content_rating': random.randint(1, 5), 'instructor_effectiveness': random.randint(1, 5), 'difficulty_level': random.randint(1, 5), 'workload_rating': random.randint(1, 5), 'recommendation_score': random.randint(1, 5), 'attendance_rate': round(random.uniform(0.6, 1.0), 2), 'assignment_quality': random.randint(1, 5), 'feedback_date': '2024-07-13', 'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S') } records.append(record) return records def save_to_csv(self, records, filename="student_feedback.csv"): file_path = self.data_dir / filename if records: fieldnames = list(records[0].keys()) with open(file_path, 'w', newline='', encoding='utf-8') as csvfile: writer = csv.DictWriter(csvfile, fieldnames=fieldnames) writer.writeheader() writer.writerows(records) print(f"💾 Saved {len(records)} records to {file_path}") return file_path def extract_data(self, num_records=100): print("🚀 Starting data extraction...") records = self.create_sample_data(num_records) csv_path = self.save_to_csv(records) print("✅ Data extraction completed!") return { 'records': records, 'csv_path': csv_path } if __name__ == "__main__": extractor = StudentFeedbackExtractor() result = extractor.extract_data() print(f"\n📊 Sample records:") for i, record in enumerate(result['records'][:3]): print(f"Record {i+1}: {record}") EOF
python3 src/extract/data_extractor.py
cat > src/transform/data_transformer.py <<'EOF' import csv from pathlib import Path class StudentFeedbackTransformer: def __init__(self): self.input_dir = Path("data/raw") self.output_dir = Path("data/processed") self.output_dir.mkdir(parents=True, exist_ok=True) print("🔄 Data transformer initialized") def load_raw_data(self, filename="student_feedback.csv"): file_path = self.input_dir / filename if not file_path.exists(): print(f"❌ File not found: {file_path}") return None records = [] with open(file_path, 'r', encoding='utf-8') as csvfile: reader = csv.DictReader(csvfile) for row in reader: records.append(row) print(f"📖 Loaded {len(records)} records") return records def enhance_data(self, records): print("➕ Enhancing data with calculated fields...") enhanced_records = [] for record in records: # Convert to numbers overall = int(record['overall_rating']) content = int(record['course_content_rating']) instructor = int(record['instructor_effectiveness']) recommendation = int(record['recommendation_score']) # Calculate satisfaction score satisfaction_score = (overall + content + instructor + recommendation) / 4 record['satisfaction_score'] = round(satisfaction_score, 2) # Add difficulty category difficulty = int(record['difficulty_level']) if difficulty <= 2: record['difficulty_category'] = 'Easy' elif difficulty == 3: record['difficulty_category'] = 'Moderate' elif difficulty == 4: record['difficulty_category'] = 'Hard' else: record['difficulty_category'] = 'Very Hard' # Add performance category if satisfaction_score >= 4: record['performance_category'] = 'Excellent' elif satisfaction_score >= 3: record['performance_category'] = 'Good' else: record['performance_category'] = 'Poor' enhanced_records.append(record) print(f"✅ Enhanced {len(enhanced_records)} records") return enhanced_records def save_processed_data(self, records, filename="processed_feedback.csv"): file_path = self.output_dir / filename if records: fieldnames = list(records[0].keys()) with open(file_path, 'w', newline='', encoding='utf-8') as csvfile: writer = csv.DictWriter(csvfile, fieldnames=fieldnames) writer.writeheader() writer.writerows(records) print(f"💾 Saved processed data to {file_path}") return file_path def transform(self): print("🚀 Starting data transformation...") # Load raw data raw_records = self.load_raw_data() if not raw_records: return None # Enhance data enhanced_records = self.enhance_data(raw_records) # Save processed data processed_file = self.save_processed_data(enhanced_records) print("✅ Data transformation completed!") return { 'processed_records': enhanced_records, 'processed_file': processed_file } if __name__ == "__main__": transformer = StudentFeedbackTransformer() result = transformer.transform() if result: print(f"\n📊 Sample enhanced record:") sample = result['processed_records'][0] for key, value in sample.items(): print(f" {key}: {value}") EOF
python3 src/transform/data_transformer.py
cat > scripts/simple_analysis.py <<'EOF' import csv from pathlib import Path from collections import Counter def load_data(): file_path = Path("data/processed/processed_feedback.csv") if not file_path.exists(): print("❌ No processed data found. Run transformer first!") return None records = [] with open(file_path, 'r', encoding='utf-8') as csvfile: reader = csv.DictReader(csvfile) for row in reader: records.append(row) return records def analyze_data(records): print("📊 ACADEMIC PULSE ANALYSIS") print("=" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 40) print(f"Total Records: {len(records)}") # Basic stats satisfaction_scores = [float(r['satisfaction_score']) for r in records] avg_satisfaction = sum(satisfaction_scores) / len(satisfaction_scores) print(f"Average Satisfaction: {avg_satisfaction:.2f}/5") # Performance distribution performance_dist = Counter(r['performance_category'] for r in records) print(f"\n🏆 Performance Distribution:") for category, count in performance_dist.items(): pct = (count / len(records)) config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 100 print(f" {category}: {count} ({pct:.1f}%)") # Difficulty distribution difficulty_dist = Counter(r['difficulty_category'] for r in records) print(f"\n⚡ Difficulty Distribution:") for category, count in difficulty_dist.items(): pct = (count / len(records)) config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 100 print(f" {category}: {count} ({pct:.1f}%)") # Top instructors instructor_scores = {} instructor_counts = {} for record in records: instructor = record['instructor_id'] score = float(record['satisfaction_score']) if instructor not in instructor_scores: instructor_scores[instructor] = 0 instructor_counts[instructor] = 0 instructor_scores[instructor] += score instructor_counts[instructor] += 1 print(f"\n🥇 Top Instructors:") instructor_averages = [] for instructor in instructor_scores: avg = instructor_scores[instructor] / instructor_counts[instructor] instructor_averages.append((instructor, avg, instructor_counts[instructor])) instructor_averages.sort(key=lambda x: x[1], reverse=True) for i, (instructor, avg, count) in enumerate(instructor_averages[:5], 1): print(f" {i}. {instructor}: {avg:.2f}/5 (n={count})") def main(): records = load_data() if records: analyze_data(records) else: print("Run the extractor and transformer first:") print(" python3 src/extract/data_extractor.py") print(" python3 src/transform/data_transformer.py") if __name__ == "__main__": main() EOF
echo "🎓 Running Academic Pulse ETL Pipeline"
cat > scripts/generate_report.py <<'EOF' """ Academic Pulse Comprehensive Report Generator =========================================== Generates a detailed report with insights and recommendations. """ import csv from pathlib import Path from collections import Counter, defaultdict from datetime import datetime def load_processed_data(): """Load processed feedback data""" file_path = Path("data/processed/processed_feedback.csv") if not file_path.exists(): print("❌ No processed data found!") return None records = [] with open(file_path, 'r', encoding='utf-8') as csvfile: reader = csv.DictReader(csvfile) for row in reader: records.append(row) return records def generate_executive_summary(records): """Generate executive summary""" total_records = len(records) satisfaction_scores = [float(r['satisfaction_score']) for r in records] avg_satisfaction = sum(satisfaction_scores) / len(satisfaction_scores) # Count unique entities unique_students = len(set(r['student_id'] for r in records)) unique_courses = len(set(r['course_id'] for r in records)) unique_instructors = len(set(r['instructor_id'] for r in records)) cat > scripts/generate_report.py << 'EOF' """ Academic Pulse Comprehensive Report Generator =========================================== Generates a detailed report with insights and recommendations. """ import csv from pathlib import Path from collections import Counter, defaultdict from datetime import datetime def load_processed_data(): """Load processed feedback data""" file_path = Path("data/processed/processed_feedback.csv") if not file_path.exists(): print("❌ No processed data found!") return None records = [] with open(file_path, 'r', encoding='utf-8') as csvfile: reader = csv.DictReader(csvfile) for row in reader: records.append(row) return records def generate_executive_summary(records): """Generate executive summary""" total_records = len(records) satisfaction_scores = [float(r['satisfaction_score']) for r in records] avg_satisfaction = sum(satisfaction_scores) / len(satisfaction_scores) # Count unique entities unique_students = len(set(r['student_id'] for r in records)) unique_courses = len(set(r['course_id'] for r in records)) unique_instructors = len(set(r['instructor_id'] for r in records)) print("📋 EXECUTIVE SUMMARY") print("=" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 50) print(f"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}") print(f"Analysis Period: Academic Year 2024") print() print(f"📊 Dataset Overview:") print(f" • Total Feedback Records: {total_records:,}") print(f" • Students Surveyed: {unique_students}") print(f" • Courses Evaluated: {unique_courses}") print(f" • Instructors Assessed: {unique_instructors}") print() print(f"📈 Key Performance Indicators:") print(f" • Overall Satisfaction Score: {avg_satisfaction:.2f}/5.0") print(f" • Performance Grade: {'A' if avg_satisfaction >= 4.0 else 'B' if avg_satisfaction >= 3.5 else 'C' if avg_satisfaction >= 3.0 else 'D'}") # Calculate trend (if we had historical data, this would be real) trend = "↗️ Improving" if avg_satisfaction > 3.0 else "↘️ Declining" print(f" • Trend: {trend}") def analyze_instructor_performance(records): """Detailed instructor analysis""" print("\n🏆 INSTRUCTOR PERFORMANCE ANALYSIS") print("=" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 50) instructor_data = defaultdict(lambda: { 'ratings': [], 'courses': set(), 'semesters': set() }) for record in records: instructor = record['instructor_id'] satisfaction = float(record['satisfaction_score']) instructor_data[instructor]['ratings'].append(satisfaction) instructor_data[instructor]['courses'].add(record['course_id']) instructor_data[instructor]['semesters'].add(record['semester']) # Calculate instructor metrics instructor_metrics = [] for instructor, data in instructor_data.items(): avg_rating = sum(data['ratings']) / len(data['ratings']) consistency = 1 - (max(data['ratings']) - min(data['ratings'])) / 4 # Consistency score instructor_metrics.append({ 'id': instructor, 'avg_rating': avg_rating, 'num_reviews': len(data['ratings']), 'courses_taught': len(data['courses']), 'consistency': consistency, 'semesters_active': len(data['semesters']) }) # Sort by average rating instructor_metrics.sort(key=lambda x: x['avg_rating'], reverse=True) print("Top Performing Instructors:") print("┌────────────┬─────────┬─────────┬─────────┬─────────────┐") print("│ Instructor │ Rating │ Reviews │ Courses │ Consistency │") print("├────────────┼─────────┼─────────┼─────────┼─────────────┤") for i, instructor in enumerate(instructor_metrics[:5], 1): rating_stars = "★" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations int(instructor['avg_rating']) + "☆" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations (5 - int(instructor['avg_rating'])) consistency_pct = instructor['consistency'] config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 100 print(f"│ {instructor['id']:<10} │ {instructor['avg_rating']:5.2f} │ {instructor['num_reviews']:7} │ {instructor['courses_taught']:7} │ {consistency_pct:8.1f}% │") print("└────────────┴─────────┴─────────┴─────────┴─────────────┘") # Identify areas for improvement low_performers = [i for i in instructor_metrics if i['avg_rating'] < 2.5] if low_performers: print(f"\n⚠️ Instructors Needing Support ({len(low_performers)} total):") for instructor in low_performers: print(f" • {instructor['id']}: {instructor['avg_rating']:.2f}/5 ({instructor['num_reviews']} reviews)") def analyze_course_performance(records): """Detailed course analysis""" print("\n📚 COURSE PERFORMANCE ANALYSIS") print("=" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 50) course_data = defaultdict(lambda: { 'satisfaction_scores': [], 'difficulty_scores': [], 'instructors': set(), 'semesters': set() }) for record in records: course = record['course_id'] course_data[course]['satisfaction_scores'].append(float(record['satisfaction_score'])) course_data[course]['difficulty_scores'].append(int(record['difficulty_level'])) course_data[course]['instructors'].add(record['instructor_id']) course_data[course]['semesters'].add(record['semester']) # Calculate course metrics course_metrics = [] for course, data in course_data.items(): avg_satisfaction = sum(data['satisfaction_scores']) / len(data['satisfaction_scores']) avg_difficulty = sum(data['difficulty_scores']) / len(data['difficulty_scores']) course_metrics.append({ 'id': course, 'avg_satisfaction': avg_satisfaction, 'avg_difficulty': avg_difficulty, 'num_reviews': len(data['satisfaction_scores']), 'instructors_count': len(data['instructors']), 'semesters_offered': len(data['semesters']) }) # Sort by satisfaction course_metrics.sort(key=lambda x: x['avg_satisfaction'], reverse=True) print("Top Performing Courses:") print("┌───────────┬─────────────┬────────────┬─────────┬─────────────┐") print("│ Course │ Satisfaction│ Difficulty │ Reviews │ Instructors │") print("├───────────┼─────────────┼────────────┼─────────┼─────────────┤") for course in course_metrics[:5]: difficulty_text = ["Easy", "Easy", "Moderate", "Hard", "Very Hard"][int(course['avg_difficulty'])-1] print(f"│ {course['id']:<9} │ {course['avg_satisfaction']:11.2f} │ {difficulty_text:<10} │ {course['num_reviews']:7} │ {course['instructors_count']:11} │") print("└───────────┴─────────────┴────────────┴─────────┴─────────────┘") def analyze_trends_and_patterns(records): """Analyze trends and patterns""" print("\n📈 TRENDS & PATTERNS ANALYSIS") print("=" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 50) # Semester analysis semester_data = defaultdict(list) for record in records: semester_data[record['semester']].append(float(record['satisfaction_score'])) print("Semester Performance:") for semester in sorted(semester_data.keys()): avg_score = sum(semester_data[semester]) / len(semester_data[semester]) count = len(semester_data[semester]) print(f" • {semester}: {avg_score:.2f}/5 ({count} reviews)") # Difficulty vs Satisfaction correlation difficulty_satisfaction = defaultdict(list) for record in records: difficulty_satisfaction[record['difficulty_category']].append(float(record['satisfaction_score'])) print("\nDifficulty vs Satisfaction Correlation:") for difficulty in ['Easy', 'Moderate', 'Hard', 'Very Hard']: if difficulty in difficulty_satisfaction: scores = difficulty_satisfaction[difficulty] avg_score = sum(scores) / len(scores) count = len(scores) print(f" • {difficulty} courses: {avg_score:.2f}/5 ({count} courses)") def generate_recommendations(records): """Generate actionable recommendations""" print("\n💡 STRATEGIC RECOMMENDATIONS") print("=" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 50) satisfaction_scores = [float(r['satisfaction_score']) for r in records] avg_satisfaction = sum(satisfaction_scores) / len(satisfaction_scores) print("🎯 Priority Actions:") if avg_satisfaction < 3.0: print(" 1. 🚨 URGENT: Overall satisfaction below acceptable threshold") print(" → Implement immediate instructor training programs") print(" → Review course content and delivery methods") elif avg_satisfaction < 3.5: print(" 1. ⚠️ Moderate improvement needed in overall satisfaction") print(" → Focus on instructor development initiatives") print(" → Enhance student support services") else: print(" 1. ✅ Overall satisfaction is good, focus on excellence") print(" → Share best practices from top performers") print(" → Implement advanced teaching methodologies") # Find performance gaps performance_dist = Counter(r['performance_category'] for r in records) poor_percentage = (performance_dist.get('Poor', 0) / len(records)) config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 100 if poor_percentage > 30: print(" 2. 📊 High percentage of poor-performing courses detected") print(f" → {poor_percentage:.1f}% of courses rated as 'Poor'") print(" → Implement targeted intervention programs") print("\n🔄 Continuous Improvement:") print(" • Establish monthly feedback review cycles") print(" • Create peer mentoring programs for instructors") print(" • Implement student success tracking systems") print(" • Develop course content refresh schedules") print("\n📊 Metrics to Monitor:") print(" • Instructor satisfaction scores (target: >3.5)") print(" • Course completion rates") print(" • Student engagement levels") print(" • Semester-over-semester improvement trends") def main(): """Generate comprehensive report""" print("🎓 ACADEMIC PULSE COMPREHENSIVE REPORT") print("=" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 60) records = load_processed_data() if not records: print("Please run the ETL pipeline first!") return # Generate all sections generate_executive_summary(records) analyze_instructor_performance(records) analyze_course_performance(records) analyze_trends_and_patterns(records) generate_recommendations(records) print("\n" + "=" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 60) print("📋 Report Generation Complete!") print("💾 Consider saving this output for stakeholder review") print("🔄 Schedule regular report generation for ongoing monitoring") if __name__ == "__main__": main() EOF
cat > scripts/generate_report.py <<'EOF' """ Academic Pulse Comprehensive Report Generator =========================================== Generates a detailed report with insights and recommendations. """ import csv from pathlib import Path from collections import Counter, defaultdict from datetime import datetime def load_processed_data(): """Load processed feedback data""" file_path = Path("data/processed/processed_feedback.csv") if not file_path.exists(): print("❌ No processed data found!") return None records = [] with open(file_path, 'r', encoding='utf-8') as csvfile: reader = csv.DictReader(csvfile) for row in reader: records.append(row) return records def generate_executive_summary(records): """Generate executive summary""" total_records = len(records) satisfaction_scores = [float(r['satisfaction_score']) for r in records] avg_satisfaction = sum(satisfaction_scores) / len(satisfaction_scores) # Count unique entities unique_students = len(set(r['student_id'] for r in records)) unique_courses = len(set(r['course_id'] for r in records)) unique_instructors = len(set(r['instructor_id'] for r in records)) print("📋 EXECUTIVE SUMMARY") print("=" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 50) print(f"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}") print(f"Analysis Period: Academic Year 2024") print() print(f"📊 Dataset Overview:") print(f" • Total Feedback Records: {total_records:,}") print(f" • Students Surveyed: {unique_students}") print(f" • Courses Evaluated: {unique_courses}") print(f" • Instructors Assessed: {unique_instructors}") print() print(f"📈 Key Performance Indicators:") print(f" • Overall Satisfaction Score: {avg_satisfaction:.2f}/5.0") print(f" • Performance Grade: {'A' if avg_satisfaction >= 4.0 else 'B' if avg_satisfaction >= 3.5 else 'C' if avg_satisfaction >= 3.0 else 'D'}") # Calculate trend (if we had historical data, this would be real) trend = "↗️ Improving" if avg_satisfaction > 3.0 else "↘️ Declining" print(f" • Trend: {trend}") def analyze_instructor_performance(records): """Detailed instructor analysis""" print("\n🏆 INSTRUCTOR PERFORMANCE ANALYSIS") print("=" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 50) instructor_data = defaultdict(lambda: { 'ratings': [], 'courses': set(), 'semesters': set() }) for record in records: instructor = record['instructor_id'] satisfaction = float(record['satisfaction_score']) instructor_data[instructor]['ratings'].append(satisfaction) instructor_data[instructor]['courses'].add(record['course_id']) instructor_data[instructor]['semesters'].add(record['semester']) # Calculate instructor metrics instructor_metrics = [] for instructor, data in instructor_data.items(): avg_rating = sum(data['ratings']) / len(data['ratings']) consistency = 1 - (max(data['ratings']) - min(data['ratings'])) / 4 # Consistency score instructor_metrics.append({ 'id': instructor, 'avg_rating': avg_rating, 'num_reviews': len(data['ratings']), 'courses_taught': len(data['courses']), 'consistency': consistency, 'semesters_active': len(data['semesters']) }) # Sort by average rating instructor_metrics.sort(key=lambda x: x['avg_rating'], reverse=True) print("Top Performing Instructors:") print("┌────────────┬─────────┬─────────┬─────────┬─────────────┐") print("│ Instructor │ Rating │ Reviews │ Courses │ Consistency │") print("├────────────┼─────────┼─────────┼─────────┼─────────────┤") for i, instructor in enumerate(instructor_metrics[:5], 1): rating_stars = "★" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations int(instructor['avg_rating']) + "☆" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations (5 - int(instructor['avg_rating'])) consistency_pct = instructor['consistency'] config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 100 print(f"│ {instructor['id']:<10} │ {instructor['avg_rating']:5.2f} │ {instructor['num_reviews']:7} │ {instructor['courses_taught']:7} │ {consistency_pct:8.1f}% │") print("└────────────┴─────────┴─────────┴─────────┴─────────────┘") # Identify areas for improvement low_performers = [i for i in instructor_metrics if i['avg_rating'] < 2.5] if low_performers: print(f"\n⚠️ Instructors Needing Support ({len(low_performers)} total):") for instructor in low_performers: print(f" • {instructor['id']}: {instructor['avg_rating']:.2f}/5 ({instructor['num_reviews']} reviews)") def analyze_course_performance(records): """Detailed course analysis""" print("\n📚 COURSE PERFORMANCE ANALYSIS") print("=" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 50) course_data = defaultdict(lambda: { 'satisfaction_scores': [], 'difficulty_scores': [], 'instructors': set(), 'semesters': set() }) for record in records: course = record['course_id'] course_data[course]['satisfaction_scores'].append(float(record['satisfaction_score'])) course_data[course]['difficulty_scores'].append(int(record['difficulty_level'])) course_data[course]['instructors'].add(record['instructor_id']) course_data[course]['semesters'].add(record['semester']) # Calculate course metrics course_metrics = [] for course, data in course_data.items(): avg_satisfaction = sum(data['satisfaction_scores']) / len(data['satisfaction_scores']) avg_difficulty = sum(data['difficulty_scores']) / len(data['difficulty_scores']) course_metrics.append({ 'id': course, 'avg_satisfaction': avg_satisfaction, 'avg_difficulty': avg_difficulty, 'num_reviews': len(data['satisfaction_scores']), 'instructors_count': len(data['instructors']), 'semesters_offered': len(data['semesters']) }) # Sort by satisfaction course_metrics.sort(key=lambda x: x['avg_satisfaction'], reverse=True) print("Top Performing Courses:") print("┌───────────┬─────────────┬────────────┬─────────┬─────────────┐") print("│ Course │ Satisfaction│ Difficulty │ Reviews │ Instructors │") print("├───────────┼─────────────┼────────────┼─────────┼─────────────┤") for course in course_metrics[:5]: difficulty_text = ["Easy", "Easy", "Moderate", "Hard", "Very Hard"][int(course['avg_difficulty'])-1] print(f"│ {course['id']:<9} │ {course['avg_satisfaction']:11.2f} │ {difficulty_text:<10} │ {course['num_reviews']:7} │ {course['instructors_count']:11} │") print("└───────────┴─────────────┴────────────┴─────────┴─────────────┘") def analyze_trends_and_patterns(records): """Analyze trends and patterns""" print("\n📈 TRENDS & PATTERNS ANALYSIS") print("=" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 50) # Semester analysis semester_data = defaultdict(list) for record in records: semester_data[record['semester']].append(float(record['satisfaction_score'])) print("Semester Performance:") for semester in sorted(semester_data.keys()): avg_score = sum(semester_data[semester]) / len(semester_data[semester]) count = len(semester_data[semester]) print(f" • {semester}: {avg_score:.2f}/5 ({count} reviews)") # Difficulty vs Satisfaction correlation difficulty_satisfaction = defaultdict(list) for record in records: difficulty_satisfaction[record['difficulty_category']].append(float(record['satisfaction_score'])) print("\nDifficulty vs Satisfaction Correlation:") for difficulty in ['Easy', 'Moderate', 'Hard', 'Very Hard']: if difficulty in difficulty_satisfaction: scores = difficulty_satisfaction[difficulty] avg_score = sum(scores) / len(scores) count = len(scores) print(f" • {difficulty} courses: {avg_score:.2f}/5 ({count} courses)") def generate_recommendations(records): """Generate actionable recommendations""" print("\n💡 STRATEGIC RECOMMENDATIONS") print("=" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 50) satisfaction_scores = [float(r['satisfaction_score']) for r in records] avg_satisfaction = sum(satisfaction_scores) / len(satisfaction_scores) print("🎯 Priority Actions:") if avg_satisfaction < 3.0: print(" 1. 🚨 URGENT: Overall satisfaction below acceptable threshold") print(" → Implement immediate instructor training programs") print(" → Review course content and delivery methods") elif avg_satisfaction < 3.5: print(" 1. ⚠️ Moderate improvement needed in overall satisfaction") print(" → Focus on instructor development initiatives") print(" → Enhance student support services") else: print(" 1. ✅ Overall satisfaction is good, focus on excellence") print(" → Share best practices from top performers") print(" → Implement advanced teaching methodologies") # Find performance gaps performance_dist = Counter(r['performance_category'] for r in records) poor_percentage = (performance_dist.get('Poor', 0) / len(records)) config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 100 if poor_percentage > 30: print(" 2. 📊 High percentage of poor-performing courses detected") print(f" → {poor_percentage:.1f}% of courses rated as 'Poor'") print(" → Implement targeted intervention programs") print("\n🔄 Continuous Improvement:") print(" • Establish monthly feedback review cycles") print(" • Create peer mentoring programs for instructors") print(" • Implement student success tracking systems") print(" • Develop course content refresh schedules") print("\n📊 Metrics to Monitor:") print(" • Instructor satisfaction scores (target: >3.5)") print(" • Course completion rates") print(" • Student engagement levels") print(" • Semester-over-semester improvement trends") def main(): """Generate comprehensive report""" print("🎓 ACADEMIC PULSE COMPREHENSIVE REPORT") print("=" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 60) records = load_processed_data() if not records: print("Please run the ETL pipeline first!") return # Generate all sections generate_executive_summary(records) analyze_instructor_performance(records) analyze_course_performance(records) analyze_trends_and_patterns(records) generate_recommendations(records) print("\n" + "=" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 60) print("📋 Report Generation Complete!") print("💾 Consider saving this output for stakeholder review") print("🔄 Schedule regular report generation for ongoing monitoring") if __name__ == "__main__": main() EOF
python3 scripts/generate_report.py
cat > scripts/dashboard.py <<'EOF' """ Academic Pulse Dashboard ======================= Interactive dashboard for exploring the data. """ import csv from pathlib import Path from collections import Counter def main_menu(): """Display main menu""" print("\n🎓 ACADEMIC PULSE DASHBOARD") print("=" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 40) print("1. 📊 Quick Overview") print("2. 🏆 Instructor Rankings") print("3. 📚 Course Analysis") print("4. 📈 Performance Trends") print("5. 🔍 Custom Search") print("0. ❌ Exit") print("-" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 40) choice = input("Select option (0-5): ").strip() return choice def load_data(): """Load processed data""" file_path = Path("data/processed/processed_feedback.csv") if not file_path.exists(): print("❌ No data found. Run ETL pipeline first!") return None records = [] with open(file_path, 'r') as csvfile: reader = csv.DictReader(csvfile) for row in reader: records.append(row) return records def quick_overview(records): """Show quick overview""" print("\n📊 QUICK OVERVIEW") print("-" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 30) total = len(records) avg_satisfaction = sum(float(r['satisfaction_score']) for r in records) / total print(f"Total Records: {total}") print(f"Average Satisfaction: {avg_satisfaction:.2f}/5") performance_dist = Counter(r['performance_category'] for r in records) print("\nPerformance Distribution:") for category, count in performance_dist.most_common(): pct = (count / total) config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 100 bar = "█" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations int(pct / 5) print(f" {category:<10}: {bar:<20} {pct:5.1f}%") def instructor_rankings(records): """Show instructor rankings""" print("\n🏆 INSTRUCTOR RANKINGS") print("-" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 35) instructor_scores = {} instructor_counts = {} for record in records: instructor = record['instructor_id'] score = float(record['satisfaction_score']) if instructor not in instructor_scores: instructor_scores[instructor] = 0 instructor_counts[instructor] = 0 instructor_scores[instructor] += score instructor_counts[instructor] += 1 # Calculate averages and sort instructor_averages = [] for instructor in instructor_scores: avg = instructor_scores[instructor] / instructor_counts[instructor] count = instructor_counts[instructor] instructor_averages.append((instructor, avg, count)) instructor_averages.sort(key=lambda x: x[1], reverse=True) print("Rank │ Instructor │ Rating │ Reviews") print("─────┼────────────┼────────┼────────") for i, (instructor, avg, count) in enumerate(instructor_averages, 1): stars = "★" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations int(avg) + "☆" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations (5 - int(avg)) print(f"{i:4} │ {instructor:<10} │ {avg:6.2f} │ {count:7}") def custom_search(records): """Custom search functionality""" print("\n🔍 CUSTOM SEARCH") print("-" config data freeCodeCamp my_first_script.py notebooks README.md reports scripts src tests test_setup.py visualizations 25) print("1. Search by Instructor") print("2. Search by Course") print("3. Search by Semester") choice = input("Select search type (1-3): ").strip() if choice == "1": instructor_id = input("Enter instructor ID (e.g., INST01): ").strip().upper() matching = [r for r in records if r['instructor_id'] == instructor_id] if matching: avg_score = sum(float(r['satisfaction_score']) for r in matching) / len(matching) print(f"\n{instructor_id} Results:") print(f" Reviews: {len(matching)}") print(f" Average Rating: {avg_score:.2f}/5") courses = set(r['course_id'] for r in matching) print(f" Courses Taught: {', '.join(sorted(courses))}") else: print(f"No records found for {instructor_id}") elif choice == "2": course_id = input("Enter course ID (e.g., COURSE01): ").strip().upper() matching = [r for r in records if r['course_id'] == course_id] if matching: avg_score = sum(float(r['satisfaction_score']) for r in matching) / len(matching) avg_difficulty = sum(int(r['difficulty_level']) for r in matching) / len(matching) print(f"\n{course_id} Results:") print(f" Reviews: {len(matching)}") print(f" Average Satisfaction: {avg_score:.2f}/5") print(f" Average Difficulty: {avg_difficulty:.1f}/5") instructors = set(r['instructor_id'] for r in matching) print(f" Instructors: {', '.join(sorted(instructors))}") else: print(f"No records found for {course_id}") elif choice == "3": semester = input("Enter semester (e.g., Fall2024): ").strip() matching = [r for r in records if semester.lower() in r['semester'].lower()] if matching: avg_score = sum(float(r['satisfaction_score']) for r in matching) / len(matching) print(f"\n{semester} Results:") print(f" Reviews: {len(matching)}") print(f" Average Satisfaction: {avg_score:.2f}/5") courses = len(set(r['course_id'] for r in matching)) instructors = len(set(r['instructor_id'] for r in matching)) print(f" Courses Offered: {courses}") print(f" Active Instructors: {instructors}") else: print(f"No records found for {semester}") def main(): """Main dashboard function""" records = load_data() if not records: return while True: choice = main_menu() if choice == "0": print("👋 Goodbye!") break elif choice == "1": quick_overview(records) elif choice == "2": instructor_rankings(records) elif choice == "3": print("\n📚 Course analysis feature coming soon!") elif choice == "4": print("\n📈 Trends analysis feature coming soon!") elif choice == "5": custom_search(records) else: print("❌ Invalid option. Please try again.") input("\nPress Enter to continue...") if __name__ == "__main__": main() EOF
echo "📋 Generating comprehensive report..."
cat > .gitignore <<'EOF' # Python __pycache__/ *.py[cod] *$py.class *.so .Python env/ venv/ ENV/ # Data files (keep sample data) # Uncomment next line if you don't want to upload data files # data/ # IDE .vscode/ .idea/ *.swp *.swo # OS .DS_Store Thumbs.db # Logs *.log # Database files *.db *.sqlite # Temporary files *.tmp *.temp test_everything.py EOF
cat > README.md <<'EOF' # 🎓 Academic Pulse ETL & Analytics Project [![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://www.python.org/downloads/) [![ETL Pipeline](https://img.shields.io/badge/Pipeline-ETL-orange.svg)]() [![Analytics](https://img.shields.io/badge/Analytics-Dashboard-green.svg)]() A comprehensive ETL (Extract, Transform, Load) pipeline for analyzing student feedback data to provide actionable insights for academic institutions. ## 🌟 Features - **📥 Data Extraction**: Generate realistic student feedback datasets with 500+ records - **🔄 Data Transformation**: Clean, validate, and enhance data with derived metrics - **📊 Advanced Analytics**: Multi-dimensional analysis with statistical insights - **🎛️ Interactive Dashboard**: Search and explore data with real-time filtering - **📋 Executive Reporting**: Comprehensive reports with actionable recommendations - **📈 Performance Tracking**: Instructor rankings and course effectiveness metrics ## 🚀 Quick Start ### Run the Complete ETL Pipeline ```bash # 1. Extract data python3 src/extract/data_extractor.py # 2. Transform data python3 src/transform/data_transformer.py # 3. Analyze data python3 scripts/simple_analysis.py # 4. Launch interactive dashboard python3 scripts/dashboard.py python3 scripts/generate_report.py 🎓 ACADEMIC PULSE ANALYSIS ======================================== Total Records: 100 Average Satisfaction: 3.08/5 🏆 Performance Distribution: Good: 51 (51.0%) Poor: 37 (37.0%) Excellent: 12 (12.0%) 🥇 Top Instructors: 1. INST02: 3.25/5 (n=22) 2. INST04: 3.18/5 (n=17) 3. INST03: 3.12/5 (n=22) 🎓 ACADEMIC PULSE ANALYSIS ======================================== Total Records: 100 Average Satisfaction: 3.08/5 🏆 Performance Distribution: Good: 51 (51.0%) Poor: 37 (37.0%) Excellent: 12 (12.0%) 🥇 Top Instructors: 1. INST02: 3.25/5 (n=22) 2. INST04: 3.18/5 (n=17) 3. INST03: 3.12/5 (n=22) 🎓 ACADEMIC PULSE DASHBOARD ======================================== 1. 📊 Quick Overview 2. �� Instructor Rankings 3. 📚 Course Analysis 4. 📈 Performance Trends 5. 🔍 Custom Search 0. ❌ Exit 0 0000000000000000 academic-pulse-etl/ ├── src/ │ ├── extract/ │ │ └── data_extractor.py # Data extraction module │ ├── transform/ │ │ └── data_transformer.py # Data transformation module │ └── analyze/ │ └── analytics.py # Analysis module ├── scripts/ │ ├── simple_analysis.py # Quick data overview │ ├── dashboard.py # Interactive dashboard │ └── generate_report.py # Comprehensive reporting ├── data/ │ ├── raw/ # Raw extracted data │ └── processed/ # Cleaned and enhanced data └── README.md # Project documentation ### Step 4: Create License ```bash cat > LICENSE << 'EOF' MIT License Copyright (c) 2025 Academic Pulse ETL Project Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. EOF
ls --color=auto -la
ls --color=auto -la
echo "📁 Checking files:"
git add -A
find . -name "*.py" -type f
echo "# Python __pycache__/ *.py[cod] *.so .Python env/ venv/ # Data files # data/ # IDE .vscode/ .idea/ # OS .DS_Store Thumbs.db # Logs *.log # Temporary files *.tmp" > .gitignore
ls --color=auto -la README.md .gitignore LICENSE
rm -rf .git
